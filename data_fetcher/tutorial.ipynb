{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据获取部分教程\n",
    "\n",
    "### 任务要求：\n",
    " * 创建一个名为`hello_world`的文献集\n",
    " * 分别在IEEE以\"good evening\"为检索词，在Scopus以\"good morning\"为检索词，获取检索结果前5条（两个数据源共10条）文献元数据，加入`hello_world`文献集\n",
    " * 获取这10篇文献的原文，保存在`./data/stay_home`目录下，并将每篇文献的原文地址记录在元数据中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行初始化，创建一些基础对象\n",
    "import os\n",
    "os.chdir('D:\\\\大三下\\\\a\\\\系统\\\\Project-KnowNet')\n",
    "from data_fetcher.id_manager import IDManager\n",
    "from data_platform.config import ConfigManager\n",
    "from data_platform.datasource.mongodb import MongoDBDS\n",
    "from pathlib import Path\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "current_location = Path(os.getcwd())\n",
    "config = ConfigManager({\n",
    "    'init':{\n",
    "        'uri': None,\n",
    "        'database': 'db'\n",
    "    }\n",
    "})\n",
    "\n",
    "mgdbds = MongoDBDS(config=config)\n",
    "# 先将数据库原有内容清空\n",
    "mgdbds.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['id_inc']\n[{'_id': 'paper_id', 'sequence_value': 0}]\n"
    }
   ],
   "source": [
    "# 创建一个ID管理器，用于给文献赋ID\n",
    "pim = IDManager(\n",
    "    config=config, \n",
    "    key=('paper_id', 'title'), \n",
    "    auto_inc = ('id_inc', 'paper_id')\n",
    ")\n",
    "\n",
    "print(mgdbds.get_db().list_collection_names())\n",
    "print(list(mgdbds.get_db()['id_inc'].find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "crawling metadata page: 0 / 1\nINFO:root:finished crawling metadata pages.\nINFO:root:and get metadata of 100 papers in all.\nINFO:root:adding paper with id 0 in paper_set hello_world\nINFO:root:paper_set hello_world not found, creating a new one.\nINFO:root:adding paper with id 1 in paper_set hello_world\nINFO:root:adding paper with id 2 in paper_set hello_world\nINFO:root:adding paper with id 3 in paper_set hello_world\nINFO:root:adding paper with id 4 in paper_set hello_world\n"
    }
   ],
   "source": [
    "# 先爬IEEE\n",
    "from data_fetcher.ieee.ieee_retrieval import IEEERetrieval\n",
    "ir = IEEERetrieval(\n",
    "    query = 'good evening',\n",
    "    offset = 0,\n",
    "    num_result = 5, \n",
    "    paper_id_manager=pim,\n",
    "    paper_set='hello_world'\n",
    ")\n",
    "ir_res = ir.retrieve()     # 执行检索\n",
    "ir.save(mgdbds)         # 将检索结果记录在数据库中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "ERROR:root:IEEEFulltextSpider, articleNumber = 4091621 PDF URL not found. Exception: list index out of range\nERROR:root:IEEEFulltextSpider, articleNumber = 7899781 error when downloading PDF. Exception: [Errno 2] No such file or directory: './data/IEEE_PDF/7899781.pdf'\nERROR:root:IEEEFulltextSpider, articleNumber = 7231357 error when downloading PDF. Exception: [Errno 2] No such file or directory: './data/IEEE_PDF/7231357.pdf'\nERROR:root:IEEEFulltextSpider, articleNumber = 7231359 error when downloading PDF. Exception: [Errno 2] No such file or directory: './data/IEEE_PDF/7231359.pdf'\nERROR:root:IEEEFulltextSpider, articleNumber = 7223720 error when downloading PDF. Exception: [Errno 2] No such file or directory: './data/IEEE_PDF/7223720.pdf'\n['paper_id', 'paper_set', 'metadata', 'id_inc']\n[{'_id': 0, 'title': 'Diurnal Variations of Atmospheric Noise in the Evening Transition Period'}, {'_id': 1, 'title': 'Towards Miss Universe automatic prediction: The evening gown competition'}, {'_id': 2, 'title': 'Circuits evening panel discussion 1: Is university circuit design research and education keeping up with industry needs?'}, {'_id': 3, 'title': 'Technology/circuits joint evening panel discussion semiconductor industry in 2020: Evolution or revolution?'}, {'_id': 4, 'title': 'Technology / circuits joint evening panel discussion semiconductor industry in 2020: Evolution or revolution? Tuesday, June 16, 20:00–22:00'}]\n[{'_id': ObjectId('5ea420fdf83e1a790820defa'), 'set_name': 'hello_world', 'paper': [0, 1, 2, 3, 4]}]\n{'_id': 4, 'title': 'Technology / circuits joint evening panel discussion semiconductor industry in 2020: Evolution or revolution? Tuesday, June 16, 20:00–22:00', 'IEEEArticleNumber': '7223720', 'abstract': 'Emerging markets such as IoT, M2M, and Big Data analysis will change the game rules of semiconductor industry in 2020. What kind of business models will be required for the players? It is becoming difficult for the Integrated Device Manufacturers (IDM) to make profits simply by fabricating devices. Not only the hardware, but services or solutions becomes more and more important. On the other hand,...', 'publication': '2015 Symposium on VLSI Technology (VLSI Technology)', 'year': '2015', 'volume': None, 'issue': None, 'doi': '10.1109/VLSIT.2015.7223720', 'type': 'conference', 'authors': [], 'id': 4, 'source': 'IEEE', 'uri': None, 'content': None, 'keywords': None, 'month': None}\n[{'_id': 'paper_id', 'sequence_value': 5}]\n"
    }
   ],
   "source": [
    "from data_fetcher.ieee.ieee_fulltext_spider import IEEEFulltextSpider\n",
    "\n",
    "article_numbers = [item['IEEEArticleNumber'] for item in ir_res]\n",
    "\n",
    "for article_number in article_numbers:\n",
    "    # 请注意：一定要在校园网环境下爬才能成功！\n",
    "    ifs = IEEEFulltextSpider(\n",
    "        article_number=article_number,\n",
    "        request_interval=5\n",
    "    )\n",
    "    ifs_result = ifs.execute() # 爬取PDF，记录爬取结果所在路径\n",
    "    # 当然并不是所有原文都能成功爬到的，爬不到就会输出ERROR的log\n",
    "    \n",
    "    # 更新数据库中对应元数据的uri字段。\n",
    "    # 可以整合到FullTextSpider类中，但这样会增加耦合，所以我还在思考\n",
    "    if ifs_result:\n",
    "        mgdbds.query_and_update_doc(\n",
    "            docset='metadata',\n",
    "            query={'IEEEArticleNumber': article_number},\n",
    "            val={'$set': {'uri': ifs_result}}\n",
    "        )\n",
    "    # 另外以后可以考虑改成多线程，这样爬IEEE的时候还能继续运行后面的程序\n",
    "\n",
    "# 此时./data中应该已经有几篇pdf了。\n",
    "# 检查现在数据库中的内容\n",
    "print(mgdbds.get_db().list_collection_names())\n",
    "print(list(mgdbds.get_db()['paper_id'].find()))\n",
    "print(list(mgdbds.get_db()['paper_set'].find()))\n",
    "print(list(mgdbds.get_db()['metadata'].find())[-1]) # 注意uri字段\n",
    "print(list(mgdbds.get_db()['id_inc'].find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:data_fetcher.dependencies.elsapy.elssearch:Module loaded.\nINFO:data_fetcher.dependencies.elsapy.elsclient:Module loaded.\nINFO:data_fetcher.dependencies.elsapy.elsclient:Sending GET request to https://api.elsevier.com/content/search/scopus?query=good%20morning\nINFO:root:Number of results got with query good%20morning: 25\n['10.1080/20008198.2020.1723857', '10.1038/s41598-020-61386-4', '10.1038/s41533-020-0163-5', '10.1038/s41598-020-57976-x', '10.1038/s41598-020-57661-z']\n"
    }
   ],
   "source": [
    "# 再爬Scopus\n",
    "from data_fetcher.scopus.scopus_retrieval import ScopusRetrieval\n",
    "\n",
    "# 爬之前要先在./data_fetcher/scopus路径下设置config文件，详见./data_fetcher/README.md\n",
    "sr = ScopusRetrieval(query='good morning', num_result=5)  # 初始化Scopus检索接口类\n",
    "sr.retrieve()       # 执行检索\n",
    "sr_doi_list = sr.get_doi_list()     # 获取检索结果中的doi，方便后续爬元数据和全文\n",
    "print(sr_doi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:data_fetcher.dependencies.elsapy.elsentity:Module loaded.\nINFO:data_fetcher.dependencies.elsapy.elsdoc:Module loaded.\nINFO:data_fetcher.dependencies.elsapy.elsclient:Sending GET request to https://api.elsevier.com/content/abstract/doi/10.1080/20008198.2020.1723857\nINFO:data_fetcher.dependencies.elsapy.elsentity:Data loaded for https://api.elsevier.com/content/abstract/doi/10.1080/20008198.2020.1723857\nINFO:root:adding paper with id 63 in paper_set hello_world\nINFO:data_fetcher.dependencies.elsapy.elsclient:Sending GET request to https://api.elsevier.com/content/article/doi/10.1080/20008198.2020.1723857\nWARNING:data_fetcher.dependencies.elsapy.elsentity:HTTP 404 Error from https://api.elsevier.com/content/article/doi/10.1080/20008198.2020.1723857\nand using headers {'X-ELS-APIKey': '4719fe9f53c1bc699307a4f4c4ccf988', 'User-Agent': 'elsapy-v0.4.6', 'Accept': 'application/json'}:\n{\"service-error\":{\"status\":{\"statusCode\":\"RESOURCE_NOT_FOUND\",\"statusText\":\"The resource specified cannot be found.\"}}}\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9a4afa5bbd76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# {\"status\":{\"statusCode\":\"RESOURCE_NOT_FOUND\",\"statusText\":\"The resource specified cannot be found.\"}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScopusFulltextSpider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr_doi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0msfs_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msfs_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\大三下\\a\\系统\\Project-KnowNet\\data_fetcher\\scopus\\scopus_fulltext_spider.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ScopusFulltextSpider, getting doi = %s failed. Exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "from data_fetcher.scopus.scopus_metadata_spider import ScopusMetadataSpider\n",
    "from data_fetcher.scopus.scopus_fulltext_spider import ScopusFulltextSpider\n",
    "\n",
    "for sr_doi in sr_doi_list:\n",
    "    # 爬元数据\n",
    "    sms = ScopusMetadataSpider(\n",
    "        doi=sr_doi,\n",
    "        paper_id_manager=pim,\n",
    "        paper_set='hello_world'\n",
    "    )\n",
    "    sms_result = sms.execute()\n",
    "    sms.save(mgdbds)\n",
    "\n",
    "    # 爬全文，更新元数据\n",
    "    # 有的文章可能没收录全文。\n",
    "    # 请注意：一定要在校园网环境下爬才能成功！\n",
    "    # 当然并不是所有原文都能成功爬到的，爬不到会返回表示错误的json: \n",
    "    # {\"status\":{\"statusCode\":\"RESOURCE_NOT_FOUND\",\"statusText\":\"The resource specified cannot be found.\"}\n",
    "    sfs = ScopusFulltextSpider(doi=sr_doi) \n",
    "    sfs_result = sfs.execute()\n",
    "\n",
    "    if sfs_result:\n",
    "         mgdbds.query_and_update_doc(\n",
    "            docset='metadata',\n",
    "            query={'doi': sr_doi},\n",
    "            val={'$set': {'uri': sfs_result}}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['paper_id', 'paper_set', 'metadata', 'id_inc']\n[{'_id': 'paper_id', 'sequence_value': 64}]\n[{'_id': ObjectId('5ea420fdf83e1a790820defa'), 'set_name': 'hello_world', 'paper': [0, 1, 2, 3, 4, 63]}]\n64\n6\n"
    }
   ],
   "source": [
    "# 最终检查数据库内容\n",
    "print(mgdbds.get_db().list_collection_names())\n",
    "print(list(mgdbds.get_db()['id_inc'].find()))\n",
    "print(list(mgdbds.get_db()['paper_set'].find()))\n",
    "# 都打印出来太长了，看个总数就行\n",
    "print(len(list(mgdbds.get_db()['paper_id'].find())))\n",
    "print(len(list(mgdbds.get_db()['metadata'].find())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:data_fetcher.dependencies.elsapy.elsclient:Sending GET request to https://api.elsevier.com/content/abstract/doi/10.1080/20008198.2020.1723857\nINFO:data_fetcher.dependencies.elsapy.elsentity:Data loaded for https://api.elsevier.com/content/abstract/doi/10.1080/20008198.2020.1723857\nINFO:data_fetcher.dependencies.elsapy.elsclient:Sending GET request to https://api.elsevier.com/content/abstract/doi/10.1038/s41598-020-61386-4\nINFO:data_fetcher.dependencies.elsapy.elsentity:Data loaded for https://api.elsevier.com/content/abstract/doi/10.1038/s41598-020-61386-4\nINFO:data_fetcher.dependencies.elsapy.elsclient:Sending GET request to https://api.elsevier.com/content/abstract/doi/10.1038/s41533-020-0163-5\nINFO:data_fetcher.dependencies.elsapy.elsentity:Data loaded for https://api.elsevier.com/content/abstract/doi/10.1038/s41533-020-0163-5\nINFO:data_fetcher.dependencies.elsapy.elsclient:Sending GET request to https://api.elsevier.com/content/abstract/doi/10.1038/s41598-020-57976-x\nINFO:data_fetcher.dependencies.elsapy.elsentity:Data loaded for https://api.elsevier.com/content/abstract/doi/10.1038/s41598-020-57976-x\nINFO:data_fetcher.dependencies.elsapy.elsclient:Sending GET request to https://api.elsevier.com/content/abstract/doi/10.1038/s41598-020-57661-z\nINFO:data_fetcher.dependencies.elsapy.elsentity:Data loaded for https://api.elsevier.com/content/abstract/doi/10.1038/s41598-020-57661-z\n"
    }
   ],
   "source": [
    "# 假设我们还要把good morning检索到的元数据添加到另一个\"hello_python\" paper_set中。\n",
    "# 爬虫是无需知道这些元数据是否已经在metadata集合或某个paper_set中出现过的（当然以后可以加上查重功能）\n",
    "# 因此会先爬一遍，再保存到metadata集合，再加入到\"hello_python\" paper_set。\n",
    "\n",
    "from data_fetcher.scopus.scopus_metadata_spider import ScopusMetadataSpider\n",
    "from data_fetcher.scopus.scopus_fulltext_spider import ScopusFulltextSpider\n",
    "\n",
    "for sr_doi in sr_doi_list:\n",
    "    # 爬元数据\n",
    "    sms = ScopusMetadataSpider(\n",
    "        doi=sr_doi,\n",
    "        paper_id_manager=pim,\n",
    "        paper_set='hello_python'\n",
    "    )\n",
    "    sms_result = sms.execute()\n",
    "    sms.save(mgdbds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['paper_id', 'paper_set', 'metadata', 'id_inc']\n[{'_id': 'paper_id', 'sequence_value': 277}]\n[{'_id': ObjectId('5ea420fdf83e1a790820defa'), 'set_name': 'hello_world', 'paper': [0, 1, 2, 3, 4, 63]}, {'_id': ObjectId('5ea4213cf83e1a790820defb'), 'set_name': 'hello_python', 'paper': [63, 98, 174, 239, 276]}]\n277\n10\n"
    }
   ],
   "source": [
    "# 最终检查数据库内容\n",
    "print(mgdbds.get_db().list_collection_names())\n",
    "print(list(mgdbds.get_db()['id_inc'].find()))\n",
    "print(list(mgdbds.get_db()['paper_set'].find()))\n",
    "# 都打印出来太长了，看个总数就行\n",
    "print(len(list(mgdbds.get_db()['paper_id'].find())))\n",
    "print(len(list(mgdbds.get_db()['metadata'].find())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36764bitd53fb91182864ec1a5d6e5d91f768daf",
   "display_name": "Python 3.6.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}